---
title: Book Recommendations via Neural Collaborative Filtering
author: Amanda Peterson-Plunkett
date: '2021-04-03'
slug: book-recommendations-using-neural-collaborative-filtering
categories: []
draft: true
tags: []
---

<link href="{{< relref "post/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html" >}}index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="{{< relref "post/2021-04-03-book-recommendations-using-neural-collaborative-filtering/index.html" >}}index_files/anchor-sections/anchor-sections.js"></script>


<p>Over the past 1.5 years I have enjoyed being part of a book club with ladies from my church.
Sadly our book club ended as life has pulled us in separate directions.
I plan to continue reading books, but what should I read?
I decided to build a recommender system.</p>
<div id="the-technical-details" class="section level2">
<h2>The technical details</h2>
<p>A recommender system utilizes user feedback history (either “explicit” such as star ratings or “implicit” such as views, clicks, purchases, etc) to recommend items to users. Recommender systems have become ubiquitous in our society. Examples include:</p>
<ul>
<li>Amazon recommends products to buy</li>
<li>Facebook recommends friends to connect with</li>
<li>YouTube recommends videos to watch</li>
</ul>
<p>In this experiment, I’ll be using a Neural Collaborative Filtering (NCF) model, which is a deep learning model originally proposed by He et. al. in their <a href="https://arxiv.org/pdf/1708.05031.pdf?source=post_page---------------------------">2017 paper</a>. The model is implemented by the authors using (Python) Keras and is available in <a href="https://github.com/hexiangnan/neural_collaborative_filtering">this GitHub repository</a>. For my own fun exercise, I implemented the model using <a href="https://keras.rstudio.com/">R Keras</a>. You can find it in my <a href="https://github.com/AmandaRP/NeuralCollaborativeFiltering">GitHub repository</a>.</p>
<p>The model is designed for binary feedback (that is, positive and negative user preferences). I’ll be using a mixture of explicit and implicit feedback, described in more details in the <a href="#bookmark">Inferring Likes and Dislikes section</a> of this post. The neural network model architecture is shown below (image from the <a href="https://arxiv.org/pdf/1708.05031.pdf?source=post_page---------------------------">NCF paper</a>).</p>
<p><img src="images/NCFarchitecture.png" width="90%" height="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="the-data" class="section level1">
<h1>The Data</h1>
<p>To train the model I used a data set originally obtained from <a href="https://en.wikipedia.org/wiki/Goodreads">GoodReads.com</a>, a social book cataloging system. The data is available on the <a href="https://sites.google.com/eng.ucsd.edu/ucsdbookgraph">USCD Book Graph webiste</a>. Specifically, I’m using three files, each containing the following fields:</p>
<ul>
<li><strong>interactions</strong>: Contains <code>user_id</code>, <code>book_id</code>, <code>is_read</code>, <code>rating</code>, <code>is_reviewed</code></li>
<li><strong>book_info</strong>: Information about the books including title, isbn, author(s), and list of shelves.</li>
<li><strong>book_id_map</strong>: books ids used to join the other two data sets together</li>
</ul>
<p>Note that this data set is a bit outdated (only six books from 2019 and none from 2020 or later). It would be nice to obtain an updated data set.</p>
<div id="filtering-by-genre" class="section level2">
<h2>Filtering by genre</h2>
<p>The <code>shelves</code> field in the <code>book_info</code> data set is a list of virtual shelves that users have added books to. The shelf name is defined by the user and can be anything such as “to read”, “book club”, or the name of the genre. By looking at the list of shelves associated with a particular book, we can make some inferences about its genre. To limit books for my book club to Christian fiction (the focus of our group), I included books that were on at least one shelf with “christian” in the name and at least one shelf with “fiction” in the name (being careful to ignore “nonfiction” and “non-fiction”). This isn’t a perfect method for determining genre, but it significantly reduced the data size: 228,648,342 user-book interactions reduced to 5,628,062, a 98% reduction. If you’d like to try filtering the data by a different genre, use the <a href="https://github.com/AmandaRP/NeuralCollaborativeFiltering/blob/master/Data/genre_subset.R"><code>genre_subset.R</code></a> script.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="images/thelionthewitchandthewardrobe.jpg" alt="The top five books appearing on the most shelves" width="12%" /><img src="images/thepoisonwoodbible.jpg" alt="The top five books appearing on the most shelves" width="12%" /><img src="images/thecroniclesofnarnia.jpg" alt="The top five books appearing on the most shelves" width="12%" /><img src="images/thefivepeopleyoumeetinheaven.jpg" alt="The top five books appearing on the most shelves" width="12%" /><img src="images/unbroken.jpg" alt="The top five books appearing on the most shelves" width="12%" />
<p class="caption">
Figure 1: The top five books appearing on the most shelves
</p>
</div>
<p>The top book, <a href="https://www.goodreads.com/book/show/100915.The_Lion_the_Witch_and_the_Wardrobe?from_search=true&amp;from_srp=true&amp;rank=1">The Lion, the Witch and the Wardrobe</a>, is a well known classic fiction book and part of the series of books at #3, <a href="https://www.goodreads.com/book/show/101704">The Chronicles of Narnia</a>. It is one that I read as a young girl. I was not familiar with the books at #2 and #4: <a href="https://www.goodreads.com/book/show/17847199">The Poisonwood Bible</a> and <a href="https://www.goodreads.com/book/show/6980106">The Five People You Meet in Heaven</a>. Finally, the last on the list at #5 is <a href="https://www.goodreads.com/book/show/21864196">Unbroken: A World War II Story of Survival, Resilience, and Redemption</a>. This book is a true story and actually shouldn’t be included in the fiction genre. So, we see that this filtering method isn’t perfect. I’m OK with that.</p>
</div>
<div id="bookmark" class="section level2">
<h2>Infering Likes and Dislikes</h2>
<p>For the NCF model, we need a list of books that each user liked and a list that they did not like. Some users provided explicit ratings (1 to 5 stars), however, not all users rate the books that they read. Additionally, it may be the case that some users just rate books that they like. To obtain a sufficient amount of training data, I included both explicit and implicit feedback.</p>
<!-- Five randomly sampled rows of the `interaction` dataframe are shown below. -->
<!-- ```{r eval = FALSE} -->
<!-- slice_sample(interactions, n=5) -->
<!-- ``` -->
<p>We’ll assume that a reader likes the book if:</p>
<ol style="list-style-type: decimal">
<li>They read the book AND (either gave it 4 or 5 stars OR didn’t provide rating). That is:
<code>(is_read == 1 &amp; (0 == rating | rating &gt;= 4))</code>, OR</li>
<li>The book is on their shelf and not read yet. That is, <code>(is_read == 0)</code>.</li>
</ol>
<p>This is an optimistic viewpoint because we may include books on the reader’s shelf that they later read and decide that they do not like.</p>
<p>Alternatively, we’ll assume that a reader did <em>not</em> like a book if they read it and gave it a rating of 1 or 2. Books with a score of 3 were not used for model training.</p>
<p>Note that there exist cases where no rating was provided (value of <code>rating</code> is 0), but a text review exists. An enhancement for obtaining more positive and negative examples might be to mine the sentiment of reviews for these instances.</p>
<p>After performing these steps, I filtered out users who did not have at least 3 “positive” book examples (a threshold that could be tweaked). As a result we have a total of 2.6489810^{5} users and 25450 books.</p>
</div>
<div id="data-for-my-book-club" class="section level2">
<h2>Data for my book club</h2>
<p>I added our club’s books to the data set. We have read dozens of books, but the following were those that we had a strong opinion about (either “good” and “bad”).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/safelyhome.jpg" alt="Books that we liked" width="11%" /><img src="images/longwaygone.jpg" alt="Books that we liked" width="11%" /><img src="images/whencricketscry.jpg" alt="Books that we liked" width="11%" /><img src="images/redeeminglove.jpg" alt="Books that we liked" width="11%" /><img src="images/Assureasdawn.jpg" alt="Books that we liked" width="11%" /><img src="images/echointhedarkness.jpg" alt="Books that we liked" width="11%" /><img src="images/avoiceinthewind.jpg" alt="Books that we liked" width="11%" /><img src="images/pearlinthesand.jpg" alt="Books that we liked" width="11%" /><img src="images/sophiesheart.jpg" alt="Books that we liked" width="11%" />
<p class="caption">
Figure 2: Books that we liked
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="images/athomeinmitford.jpg" alt="Books that we did not like" width="11%" /><img src="images/thehideaway.jpg" alt="Books that we did not like" width="11%" /><img src="images/thelacemaker.jpg" alt="Books that we did not like" width="11%" /><img src="images/theredtent.jpg" alt="Books that we did not like" width="11%" />
<p class="caption">
Figure 3: Books that we did not like
</p>
</div>
<p>As you can see, the number of “duds” was much smaller than the number of books that we liked. Francine Rivers was a popular author (I’ve only read one of her books, but hope to catch up soon). I would have also used the following books as positive training examples, but they were not in the data set:
<a href="https://www.goodreads.com/book/show/35620754-the-kremlin-conspiracy?ac=1&amp;from_search=true&amp;qid=TOx79fls5W&amp;rank=1">The Kremlin Conspiracy</a> by Joel Rosenberg
<!-- , [The 49th Mistic](https://www.goodreads.com/book/show/36548233-the-49th-mystic) by Ted Dekker,  -->
and <a href="https://www.goodreads.com/book/show/40402597-my-heart-belongs-in-the-blue-ridge?from_search=true&amp;from_srp=true&amp;qid=GIbp9eeJTH&amp;rank=1">My Heart Belongs in Blue Ridge</a> by Pepper Basham.</p>
</div>
<div id="trainvalidationtest-split" class="section level2">
<h2>Train/Validation/Test Split</h2>
<p>I split the data into train, validation, and test sets following the method used in the NCF paper:</p>
<ul>
<li>Training: 4 negatives for every positive (for each user)</li>
<li>Validation: 1 positive (for each user)</li>
<li>Test: 1 positive, 100 negative (for each user)</li>
</ul>
<p>If a user did not have enough negatives for the 4-to-1 ratio of negatives to positives in the training set, then I sampled “implicit” negatives (i.e. those books that are not on the user’s shelf). The empirical popularity distribution of the books provided sampling probabilities, meaning that more popular books were more likely to be sampled. This is often a good idea for training recommender systems since users are likely already aware of the popular items. I sampled these training data points for each epoch of model training.</p>
<p>If the user had <em>more</em> negatives than the 4-to-1 ratio, then the extras were put in the test set. Subsequently, if the test set lacked the 100 negatives, then I sampled “implicit” negatives to fill out the test set (using the same strategy as described for the training set). Sampling of the test set was done once prior to model training.</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>The distribution of predictions for all books and users in the test is shown below (separated by “good” and “bad” books).</p>
<p><img src="images/plt_preds.png" width="90%" style="display: block; margin: auto;" /></p>
<p>I’m happy to see that the most of the “bad” books have low scores while the “good” books are more evenly spread out with a little bump on the high end. The goal is to score “good” higher than the “bad” (on a per-user basis).</p>
<p>The scores can be used to rank the books for each user. A couple of useful metrics for evaluating that ranking are:</p>
<ul>
<li>Hit rate (HR) at <em>k</em>: The percentage of users who’s “good” book appeared in their top <em>k</em> recommended books. (Recall the test set is composed of 100 “bad” and 1 “good” book for each user.)</li>
<li>Normalized Discounted Cummulative Gain (NDGC) at 10: TODO</li>
</ul>
<p>The result was 0.59 HR at 10 and 0.37 NDGC at 10 (evaluated on the test set).</p>
<p>As a sanity check I also computed the metrics for a recommender “model” based solely on popularity (i.e. everyone’s number one recommendation will be “The Lion, The Witch, and The Wardrobe” unless they already have it on their shelf), then it turns out that the HR at 10 is 0.28 and the NDCG at 10 is 0.15. These metrics are significantly worse than those from the NCF model, so it paid to add the personalization.</p>
</div>
<div id="recommendations-for-my-book-club" class="section level2">
<h2>Recommendations for my book club</h2>
<p>To get a better sense of how this recommender did, let’s inspect the recommendations for my book club. The top 3 recommendations were all books by Francine Rivers (which makes sense since four of her books were on our short list of books that we liked). In fact, looking at the top 25 books on the recommended list, 76 percent were written by this author. It seems that having four of nine books by Francine as part of our “good” training set may be skewing the recommendations
toward this author.</p>
<p>Coming in at #4, we find the first book by a different author: <a href="https://www.goodreads.com/book/show/359994.Gods_and_Kings">Gods and Kings</a> by Lynn Austin. This is an author that we have not read before. Interestingly, the book appears as a “Readers also enjoyed” suggestion on the GoodReads page of one of our “liked” books:</p>
<p><img src="images/peoplealsoliked.png" width="95%" style="display: block; margin: auto;" /></p>
<p>Looking at the worst scoring books for our group, we find books that we would likely <em>not</em> enjoy: <a href="https://www.goodreads.com/book/show/22085728-come-on-over#">Come On Over (CO2)</a> and <a href="https://www.goodreads.com/book/show/256272615-marry-now-sorry-later">Marry Me Now, Sorry Later</a> (rank 25,437 and 25,436 respectively). In fact, these books are not even in our preferred genre of Christian fiction. They were mistakenly included due to the author’s name: Christian Simamora.</p>
<p>Next, I decided to look at the rank of the top recommended book for each of the authors associated with books that my group enjoyed. The result was:</p>
<ul>
<li>Francine Rivers (#1, <a href="https://www.goodreads.com/book/show/95602.Mark_of_the_Lion_Trilogy?from_search=true&amp;from_srp=true&amp;qid=aPoPk0tBes&amp;rank=1">Mark of the Lion Trilogy</a>)</li>
<li>Lori Wick (#30, <a href="https://www.goodreads.com/book/show/289745.Pretense?ac=1&amp;from_search=true&amp;qid=J93foRiGvl&amp;rank=1">Pretense</a>)</li>
<li>Charles Martin (#32, <a href="https://www.goodreads.com/book/show/241386.Wrapped_in_Rain?from_search=true&amp;from_srp=true&amp;qid=3zJt0wTUgi&amp;rank=1">Wrapped in Rain</a>)</li>
<li>Tessa Afshar (#38, <a href="https://www.goodreads.com/book/show/20702178-in-the-field-of-grace?from_search=true&amp;from_srp=true&amp;qid=ElbF3skUjQ&amp;rank=1">In the Field of Grace</a>)</li>
<li>Randy Alcorn (#425, <a href="https://www.goodreads.com/book/show/290363.Deadline?ac=1&amp;from_search=true&amp;qid=6dPXAtqezB&amp;rank=5">Deadline</a>)</li>
</ul>
<p>Interestingly, I had already ordered Tessa Afshar’s “In the Field of Grace” before looking at these recommendations. So, that is right on point. Sadly, Randy Alcorn’s highest ranking book is at number 425 (his book, “Safely Home”, was one of my favorites).</p>
<p>Overall, I’m fairly happy with these recommendations, even though the top of the list doesn’t demonstrate as much author variety as I’d like to see. The recommendations do feel tailored to my group’s reading interests.</p>
</div>
<div id="final-thoughts" class="section level2">
<h2>Final thoughts</h2>
<p>There are a few areas of improvement that could be made:</p>
<ol style="list-style-type: decimal">
<li>Due to hardware limitations and slow model training, I didn’t perform hyper-parameter tuning. (I am using a GTX 970 GPU. Each training epoch takes a few hours.) Instead I used the defaults that were used in the NCF paper. A grid search of the hyperparamter space would likely be beneficial.</li>
<li>My group’s training data is small (nine books). Compare that to Goodreads, which requires users to rate 20 books before they will provide recommendations. A model that is better suited for cold start situations (users with little to no training data) may be beneficial. Such models incorporates side features (e.g. author, book description).</li>
<li>As mentioned earlier, the Goodreads data set that I used is a couple of years out of date. It would be great to retrain the model on the newer data.</li>
<li>There are many other types of recommendation models to try, some of which claim to beat NCF. See <a href="https://github.com/hongleizhang/RSPapers">this list of must-read recommender papers</a>.</li>
</ol>
<p>In the meantime, I have a few books on my short list to read, but I’m still in search of more. I may try another recommender model. Or, let me know if you have a good suggestion! You can find me on Twitter <span class="citation">@DrAmandaRP</span>.</p>
<p>P.S. If you are new to deep learning in R, I recommend the <a href="https://www.manning.com/books/deep-learning-with-r">Deep Learning with R</a> book by François Chollet with J. J. Allaire.</p>
</div>
</div>
